{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reading ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "855598\n",
      "72784\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "class UserItemData:\n",
    "    \n",
    "    read_counter = 0\n",
    "    \n",
    "    def __init__(self, path, from_date = None, to_date = None, min_ratings = None):\n",
    "        self.path = path\n",
    "        self.from_date = from_date\n",
    "        self.to_date = to_date\n",
    "        self.min_ratings = min_ratings\n",
    "        self.data = pd.read_table(self.path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.from_date != None and self.to_date != None:\n",
    "            from_date_dt = datetime.datetime.strptime(from_date, \"%d.%m.%Y\")\n",
    "            to_date_dt = datetime.datetime.strptime(to_date, \"%d.%m.%Y\")\n",
    "#             print('From: ' +str(from_date_dt)+ '\\tTo: ' +str(to_date_dt))\n",
    "            \n",
    "            self.data['date_combined'] = pd.to_datetime(dict(year = self.data.date_year, month = self.data.date_month, day = self.data.date_day))\n",
    "            # тут, где >= from_date_dt, где < to_date_dt и > self.min_ratings - это я догадывался, что включать, а что не,\n",
    "            # чтобы мой аутпут сходился с ее аутпутом\n",
    "            self.data = self.data[(self.data.date_combined >= from_date_dt) & (self.data.date_combined < to_date_dt)]\n",
    "            \n",
    "        if self.from_date != None and self.to_date == None:\n",
    "            \n",
    "            from_date_dt = datetime.datetime.strptime(from_date, \"%d.%m.%Y\")\n",
    "            \n",
    "            self.data['date_combined'] = pd.to_datetime(dict(year = self.data.date_year, month = self.data.date_month, day = self.data.date_day))\n",
    "            self.data = self.data[(self.data.date_combined >= from_date_dt)]\n",
    "        \n",
    "        if self.from_date == None and self.to_date != None:\n",
    "            \n",
    "            to_date_dt = datetime.datetime.strptime(to_date, \"%d.%m.%Y\")\n",
    "            \n",
    "            self.data['date_combined'] = pd.to_datetime(dict(year = self.data.date_year, month = self.data.date_month, day = self.data.date_day))\n",
    "            self.data = self.data[(self.data.date_combined < to_date_dt)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.min_ratings != None:\n",
    "            self.data = self.data.groupby('movieID').filter(lambda s: s.rating.count() > self.min_ratings)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def print_data(self):\n",
    "        display(self.data)\n",
    "        \n",
    "    def nratings(self):\n",
    "        read_counter = len(self.data.index)\n",
    "#         print('Program has read ' +str(read_counter)+ ' rows.')\n",
    "        return read_counter\n",
    "    \n",
    "    def save(self, save_path):\n",
    "        self.data.to_csv(save_path, sep = '\\t', index=False)\n",
    "        \n",
    "\n",
    "test = UserItemData('data/user_ratedmovies.dat')\n",
    "# test.print_data()\n",
    "print(test.nratings())\n",
    "\n",
    "test2 = UserItemData('data/user_ratedmovies.dat', min_ratings = 100, from_date = '12.1.2007', to_date = '16.2.2008')\n",
    "# test2.print_data()\n",
    "print(test2.nratings())\n",
    "# save_path = 'output/output1.dat'\n",
    "# test2.save(save_path)\n",
    "\n",
    "# test3 = UserItemData(save_path)\n",
    "# test3.print_data()\n",
    "# print(test3.nratings())\n",
    "# test3.save('output/output2.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Reading movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy story\n"
     ]
    }
   ],
   "source": [
    "class MovieData:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data = pd.read_table(self.path, encoding='latin-1')\n",
    "    \n",
    "    def get_title(self, movieID):\n",
    "        return self.data.loc[self.data['id'] == movieID, 'title'].iloc[0]\n",
    "\n",
    "md = MovieData('data/movies.dat')\n",
    "print(md.get_title(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Random predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "Movie: Toy story, score: 3\n",
      "Movie: Grumpy Old Men, score: 5\n",
      "Movie: Money Train, score: 3\n",
      "Movie: The Usual Suspects, score: 2\n",
      "Movie: City Hall, score: 3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class RandomPredictor:\n",
    "    def __init__(self, min_rat, max_rat):\n",
    "        self.min_rat = min_rat\n",
    "        self.max_rat = max_rat\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "    def predict(self, user_id): # note that user_id is used nowhere here at all as it has no influence here because it is just a random prediction...\n",
    "        recommend_dict = {}\n",
    "        for i in self.X.data.movieID.unique(): # iterate though the list of unique movieID-s that appear in this particular data\n",
    "            recommend_dict[i] = random.randint(self.min_rat, self.max_rat)\n",
    "        return recommend_dict\n",
    "\n",
    "    \n",
    "    \n",
    "md = MovieData('data/movies.dat')\n",
    "uim = UserItemData('data/user_ratedmovies.dat')\n",
    "rp = RandomPredictor(1, 5)\n",
    "rp.fit(uim)\n",
    "pred = rp.predict(78)\n",
    "print(type(pred))\n",
    "items = [1, 3, 20, 50, 100]\n",
    "for item in items:\n",
    "    print(\"Movie: {}, score: {}\".format(md.get_title(item), pred[item]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: Congo, score: 5\n",
      "Movie: Die Hard, score: 5\n",
      "Movie: Armageddon, score: 5\n",
      "Movie: Superman, score: 5\n",
      "Movie: Hart's War, score: 5\n"
     ]
    }
   ],
   "source": [
    "class Recommender:\n",
    "    \n",
    "    def __init__(self, predictor):\n",
    "        self.predictor = predictor\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        self.predictor.fit(X)\n",
    "    \n",
    "    def recommend(self, userID, n = 10, rec_seen = True):\n",
    "        \n",
    "        pred = self.predictor.predict(userID)\n",
    "        pred_list = list(pred.items()) # we need to sort this, so dictionary will not help us here, so convert this to the list of tupples\n",
    "        pred_list_sorted = sorted(pred_list, key=lambda x: x[1], reverse = True) # sort from rating 5 to rating 1\n",
    "        recommend_list = []\n",
    "        \n",
    "        \n",
    "        if rec_seen == False:\n",
    "            table_seen = self.X.data[self.X.data.userID == userID]\n",
    "            column_seen = table_seen['movieID'].tolist()\n",
    "            \n",
    "            tup_dict = dict(pred_list_sorted) # in order to reach first element of each tupple, we convert this back to the dictionary\n",
    "            for i in column_seen:\n",
    "                tup_dict.pop(i) # remove already seen movie\n",
    "            # чувак их инета сказал, что перед тапл надо поставить лист. Я чекнул, но разницы не заметил, но все равно оставлю лист на всякий:\n",
    "            pred_list_sorted = list(tuple(tup_dict.items())) # and then in the end again convert it to the list of tupples\n",
    "        \n",
    "        \n",
    "        for i in range(n):\n",
    "            recommend_list.append(pred_list_sorted[i])\n",
    "        \n",
    "#         print(recommend_list)\n",
    "        \n",
    "        return recommend_list\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "md = MovieData('data/movies.dat')\n",
    "uim = UserItemData('data/user_ratedmovies.dat')\n",
    "rp = RandomPredictor(1, 5)\n",
    "rec = Recommender(rp)\n",
    "rec.fit(uim)\n",
    "rec_items = rec.recommend(78, n=5, rec_seen=False)\n",
    "for idmovie, val in rec_items:\n",
    "    print(\"Movie: {}, score: {}\".format(md.get_title(idmovie), val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Average predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run b = 0 :\n",
      "Movie: Brother Minister: The Assassination of Malcolm X, score: 5.0\n",
      "Movie: Synthetic Pleasures, score: 5.0\n",
      "Movie: Gabbeh, score: 5.0\n",
      "Movie: Storefront Hitchcock, score: 5.0\n",
      "Movie: Ko to tamo peva, score: 5.0\n",
      "Run b = 100 :\n",
      "Movie: The Usual Suspects, score: 4.225944245560473\n",
      "Movie: The Godfather: Part II, score: 4.146907937910189\n",
      "Movie: Cidade de Deus, score: 4.116538340205236\n",
      "Movie: The Dark Knight, score: 4.10413904093503\n",
      "Movie: 12 Angry Men, score: 4.103639627096175\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AveragePredictor:\n",
    "    \n",
    "    def __init__(self, b):\n",
    "        self.b = b\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "    \n",
    "    def avg_calc(self, n, vs):\n",
    "        avg = (vs + self.b * self.g_avg) / (n + self.b)\n",
    "        return avg\n",
    "        \n",
    "    def predict(self, user_id): # note that user_id is used nowhere here at all as it has no influence here because it is just a random prediction...\n",
    "        \n",
    "        recommend_dict = {}\n",
    "        \n",
    "        self.g_avg = self.X.data.rating.mean()\n",
    "        \n",
    "        # first, create new table with movieID and n cols:\n",
    "        new_table = self.X.data.groupby(['movieID'])['rating'].count().reset_index(name=\"n\")\n",
    "        # secondly, create another table (consisting of one col only) for another function\"\n",
    "        vs_column = self.X.data.groupby(['movieID'])['rating'].sum().reset_index(name=\"vs\")['vs']\n",
    "        # add previous column to the previous table:\n",
    "        new_table['vs'] = vs_column\n",
    "#         display(new_table)\n",
    "        \n",
    "        new_table['avg'] = np.vectorize(self.avg_calc)(new_table['n'], new_table['vs'])\n",
    "        \n",
    "        for i in new_table.movieID.unique(): # iterate though the list of unique movieID-s that appear in this particular data\n",
    "                                                # вообще-то тут нету смысла в unique, ибо new_table и так уже содержит unique movieID\n",
    "            \n",
    "            recommend_dict[i] = new_table.loc[new_table.movieID == i, 'avg'].iloc[0]\n",
    "        \n",
    "        \n",
    "        return recommend_dict\n",
    "\n",
    "\n",
    "md = MovieData('data/movies.dat')\n",
    "uim = UserItemData('data/user_ratedmovies.dat')\n",
    "print('Run b = 0 :')\n",
    "rp = AveragePredictor(b=0)\n",
    "rec = Recommender(rp)\n",
    "rec.fit(uim)\n",
    "rec_items = rec.recommend(78, n=5, rec_seen=False)\n",
    "for idmovie, val in rec_items:\n",
    "    print(\"Movie: {}, score: {}\".format(md.get_title(idmovie), val))\n",
    "\n",
    "print('Run b = 100 :')\n",
    "rp = AveragePredictor(b=100)\n",
    "rec = Recommender(rp)\n",
    "rec.fit(uim)\n",
    "rec_items = rec.recommend(78, n=5, rec_seen=False)\n",
    "for idmovie, val in rec_items:\n",
    "    print(\"Movie: {}, score: {}\".format(md.get_title(idmovie), val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Recommending the most watched movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: The Lord of the Rings: The Fellowship of the Ring, score: 1576\n",
      "Movie: The Lord of the Rings: The Two Towers, score: 1528\n",
      "Movie: The Lord of the Rings: The Return of the King, score: 1457\n",
      "Movie: The Silence of the Lambs, score: 1431\n",
      "Movie: Shrek, score: 1404\n"
     ]
    }
   ],
   "source": [
    "class ViewsPredictor:\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "    def predict(self, user_id): # note that user_id is used nowhere here at all as it has no influence here because it is just a random prediction...\n",
    "        \n",
    "        recommend_dict = {}\n",
    "        \n",
    "        # тут я специально создал новую переменную, ибо если буду перезаписывать в старую self.X.data,\n",
    "        # то потом уже после этого класса, другая часть кода будет опираться на МОДИФИЦИРОВАННЫЙ self.X.data,\n",
    "        # поэтому тут создаем свою переменную, где будут только 2 + 1 = 3 колонки\n",
    "        grouped_data = self.X.data.groupby(['movieID'])['movieID'].count().reset_index(name=\"movieID_count\")\n",
    "        \n",
    "#         display(grouped_data)\n",
    "            \n",
    "        recommend_dict = grouped_data.set_index('movieID').to_dict()['movieID_count']\n",
    "        \n",
    "        return recommend_dict\n",
    "\n",
    "md = MovieData('data/movies.dat')\n",
    "uim = UserItemData('data/user_ratedmovies.dat')\n",
    "rp = ViewsPredictor()\n",
    "rec = Recommender(rp)\n",
    "rec.fit(uim)\n",
    "rec_items = rec.recommend(78, n=5, rec_seen=False)\n",
    "for idmovie, val in rec_items:\n",
    "    print(\"Movie: {}, score: {}\".format(md.get_title(idmovie), val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Predicting scores with similarity between products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating similarities: \n",
      "Similarity between the movies 'Men in black'(1580) and 'Ghostbusters'(2716):  0.23395523176756633\n",
      "Similarity between the movies 'Men in black'(1580) and 'Schindler's List'(527):  0.0\n",
      "Similarity between the movies 'Men in black'(1580) and 'Independence day'(780):  0.42466125844687613\n",
      "Now, we will NOT calculate it, but simply GET in from the table:\n",
      "Similarity between the movies 'Men in black'(1580) and 'Ghostbusters'(2716):  0.23395523176756633\n",
      "Similarity between the movies 'Men in black'(1580) and 'Schindler's List'(527):  0.0\n",
      "Similarity between the movies 'Men in black'(1580) and 'Independence day'(780):  0.42466125844687613\n",
      "Checking vice-versa coordinates:\n",
      "Similarity between the movies 'Men in black'(1580) and 'Ghostbusters'(2716):  0.23395523176756633\n",
      "Similarity between the movies 'Men in black'(1580) and 'Schindler's List'(527):  0.0\n",
      "Similarity between the movies 'Men in black'(1580) and 'Independence day'(780):  0.42466125844687613\n",
      "Predictions for 78: \n",
      "Time needed for movieID 32: 119.55599880218506 seconds.\n",
      "Time needed for movieID 110: 124.26029229164124 seconds.\n",
      "Time needed for movieID 296: 139.01805448532104 seconds.\n",
      "Time needed for movieID 589: 120.14196181297302 seconds.\n",
      "Time needed for movieID 1036: 114.92954277992249 seconds.\n",
      "Time needed for movieID 1527: 118.85237956047058 seconds.\n",
      "Time needed for movieID 2571: 140.19770216941833 seconds.\n",
      "Time needed for movieID 2762: 119.71965646743774 seconds.\n",
      "Time needed for movieID 2959: 116.84038972854614 seconds.\n",
      "Time needed for movieID 3793: 95.86986446380615 seconds.\n",
      "Time needed for movieID 4993: 152.66987299919128 seconds.\n",
      "Time needed for movieID 5952: 147.19972109794617 seconds.\n",
      "Time needed for movieID 7153: 141.61978936195374 seconds.\n",
      "Time needed for movieID 32587: 113.50044751167297 seconds.\n",
      "Time needed for movieID 150: 87.57957935333252 seconds.\n",
      "Time needed for movieID 260: 106.01005458831787 seconds.\n",
      "Time needed for movieID 318: 101.38004994392395 seconds.\n",
      "Time needed for movieID 356: 105.099764585495 seconds.\n",
      "Time needed for movieID 480: 97.33992052078247 seconds.\n",
      "Time needed for movieID 527: 75.8802559375763 seconds.\n",
      "Time needed for movieID 541: 68.98985004425049 seconds.\n",
      "Time needed for movieID 608: 70.61961507797241 seconds.\n",
      "Time needed for movieID 780: 79.59991788864136 seconds.\n",
      "Time needed for movieID 858: 63.555542945861816 seconds.\n",
      "Time needed for movieID 1089: 60.894603967666626 seconds.\n",
      "Time needed for movieID 1097: 59.97016406059265 seconds.\n",
      "Time needed for movieID 1136: 61.720035791397095 seconds.\n",
      "Time needed for movieID 1196: 61.94966220855713 seconds.\n",
      "Time needed for movieID 1198: 58.086543560028076 seconds.\n",
      "Time needed for movieID 1210: 53.86336398124695 seconds.\n",
      "Time needed for movieID 1214: 43.85026717185974 seconds.\n",
      "Time needed for movieID 1240: 45.06000852584839 seconds.\n",
      "Time needed for movieID 1265: 40.90124177932739 seconds.\n",
      "Time needed for movieID 1270: 46.40771746635437 seconds.\n",
      "Time needed for movieID 1682: 36.59009003639221 seconds.\n",
      "Time needed for movieID 1923: 31.820350646972656 seconds.\n",
      "Time needed for movieID 2028: 32.489940881729126 seconds.\n",
      "Time needed for movieID 2716: 27.929619789123535 seconds.\n",
      "Time needed for movieID 2858: 31.32028555870056 seconds.\n",
      "Time needed for movieID 2997: 22.400028228759766 seconds.\n",
      "Time needed for movieID 3996: 21.27950644493103 seconds.\n",
      "Time needed for movieID 4226: 20.78006076812744 seconds.\n",
      "Time needed for movieID 4973: 14.840440034866333 seconds.\n",
      "Time needed for movieID 5445: 13.709625005722046 seconds.\n",
      "Time needed for movieID 6365: 9.760366678237915 seconds.\n",
      "Time needed for movieID 6711: 8.433624744415283 seconds.\n",
      "Time needed for movieID 6874: 8.065854549407959 seconds.\n",
      "Time needed for movieID 7361: 3.1945960521698 seconds.\n",
      "Time needed for movieID 7438: 0.015619754791259766 seconds.\n",
      "Time needed for movieID 1: 122.5243022441864 seconds.\n",
      "Time needed for movieID 47: 119.75337529182434 seconds.\n",
      "Time needed for movieID 50: 117.87169861793518 seconds.\n",
      "Time needed for movieID 344: 108.45511102676392 seconds.\n",
      "Time needed for movieID 364: 117.54109239578247 seconds.\n",
      "Time needed for movieID 367: 130.53523588180542 seconds.\n",
      "Time needed for movieID 377: 138.71184659004211 seconds.\n",
      "Time needed for movieID 500: 129.14947819709778 seconds.\n",
      "Time needed for movieID 597: 117.08812665939331 seconds.\n",
      "Time needed for movieID 648: 125.02442073822021 seconds.\n",
      "Time needed for movieID 1580: 141.00095534324646 seconds.\n",
      "Time needed for movieID 1721: 128.07761693000793 seconds.\n",
      "Time needed for movieID 2683: 104.65731501579285 seconds.\n",
      "Time needed for movieID 4306: 139.5139684677124 seconds.\n",
      "Time needed for movieID 4886: 115.52097296714783 seconds.\n",
      "Time needed for movieID 4963: 125.29130959510803 seconds.\n",
      "Time needed for movieID 6377: 121.64591217041016 seconds.\n",
      "Time needed for movieID 457: 112.3030755519867 seconds.\n",
      "Time needed for movieID 592: 116.83250570297241 seconds.\n",
      "Time needed for movieID 593: 143.30086016654968 seconds.\n",
      "Time needed for movieID 1961: 102.88136887550354 seconds.\n",
      "Time needed for movieID 2628: 114.64582681655884 seconds.\n",
      "Time needed for movieID 3578: 127.91875433921814 seconds.\n",
      "Time needed for movieID 5418: 106.42155337333679 seconds.\n",
      "Time needed for movieID 33794: 119.88843250274658 seconds.\n",
      "Time needed for movieID 1291: 117.73325848579407 seconds.\n",
      "Time needed for movieID 4995: 116.00227165222168 seconds.\n",
      "Time needed for movieID 5349: 128.46707940101624 seconds.\n",
      "Time needed for movieID 5989: 104.66351270675659 seconds.\n",
      "Time needed for movieID 6539: 125.69361352920532 seconds.\n",
      "Time needed for movieID 8961: 113.51423907279968 seconds.\n",
      "Time needed for movieID 1704: 110.76291632652283 seconds.\n",
      "Time spent for the whole RECOMMENDATION: 7299.167834043503 seconds.\n",
      "Movie: Shichinin no samurai, score: 4.3557347903101595\n",
      "Movie: The Usual Suspects, score: 4.3546817280678365\n",
      "Movie: The Silence of the Lambs, score: 4.335305303472517\n",
      "Movie: Sin City, score: 4.2786871668991004\n",
      "Movie: Monsters, Inc., score: 4.21758113694352\n",
      "Movie: The Incredibles, score: 4.2070985832817485\n",
      "Movie: The Lord of the Rings: The Fellowship of the Ring, score: 4.152792107348347\n",
      "Movie: Batman Begins, score: 4.146413806700199\n",
      "Movie: Die Hard, score: 4.125915602232819\n",
      "Movie: Rain Man, score: 4.071535242958552\n",
      "Movie: The Lord of the Rings: The Return of the King, score: 4.020237449257013\n",
      "Movie: A Beautiful Mind, score: 4.015142490064839\n",
      "Movie: Good Will Hunting, score: 4.0092808069228205\n",
      "Movie: The Lord of the Rings: The Two Towers, score: 3.9414763050955943\n",
      "Movie: Indiana Jones and the Last Crusade, score: 3.7969764963789245\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class ItemBasedPredictor:\n",
    "\n",
    "    def __init__(self, min_values = 0, threshold = 0):\n",
    "        self.min_values = min_values\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \n",
    "        self.X = X\n",
    "        \n",
    "        self.similarities = pd.DataFrame(index = self.X.data.movieID.unique())\n",
    "        for i in self.similarities.index:\n",
    "            self.similarities[str(i)] = np.nan\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def find_avgRating(self, userID):\n",
    "        one_movie_rating = self.X.data[self.X.data.userID == userID]['rating'].mean()\n",
    "        return one_movie_rating\n",
    "    \n",
    "    def getRatingDifference(self, userID, avgRating, p):\n",
    "        rating = self.X.data.loc[((self.X.data.userID == userID) & (self.X.data.movieID == p)), 'rating'].iloc[0]\n",
    "        return rating - avgRating\n",
    "    \n",
    "    def cols_product(self, col1, col2):\n",
    "        return col1 * col2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, userID): # а вот теперь мы этот userID наконец-то юзаем, гы\n",
    "        \n",
    "        start_time_prediction = time.time()\n",
    "        recommend_dict = {}\n",
    "        \n",
    "        unique_movies = self.X.data.movieID.unique()\n",
    "#         display(unique_movies)\n",
    "        \n",
    "        for i in unique_movies: # iterate though the list of unique movieID-s that appear in this particular data\n",
    "            \n",
    "            start_time_movie = time.time()\n",
    "            \n",
    "            personal = self.X.data[self.X.data.userID == userID][['movieID', 'rating']]\n",
    "            personal['similarity'] = np.nan\n",
    "\n",
    "            remove_index = personal[personal.movieID == i].index\n",
    "#             print('Next index to remove: ' +str(remove_index))\n",
    "            personal = personal.drop(remove_index)\n",
    "\n",
    "            personal['similarity'] = np.vectorize(self.similarity)(i, personal['movieID'])\n",
    "            personal['product'] = np.vectorize(self.cols_product)(personal['rating'], personal['similarity'])\n",
    "            \n",
    "#             display(personal)\n",
    "            numerator = personal['product'].sum()\n",
    "            denominator = personal['similarity'].sum()\n",
    "\n",
    "            result = numerator / denominator\n",
    "#             print('The recommentation for movieID ' +str(i)+ ' is: ' +str(result))\n",
    "            \n",
    "            recommend_dict[i] = result\n",
    "            \n",
    "            end_time_movie = time.time()\n",
    "            time_movie = end_time_movie - start_time_movie\n",
    "            print('Time needed for movieID ' +str(i)+ ': ' +str(time_movie)+ ' seconds.')\n",
    "        \n",
    "        end_time_prediction = time.time()\n",
    "        time_prediction = end_time_prediction - start_time_prediction\n",
    "        print('Time spent for the whole RECOMMENDATION: ' +str(time_prediction)+ ' seconds.')\n",
    "        \n",
    "        # Save similarities to use it in the future...\n",
    "        # Index must be true here so that we can further read the firct column as index column:\n",
    "        self.similarities.to_csv('output/calculated_similarities.dat', sep = '\\t', index=True)\n",
    "        \n",
    "        return recommend_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def similarity(self, p1, p2):\n",
    "        \n",
    "        # Check if this is NaN value:\n",
    "        if pd.isna(self.similarities.at[p1, str(p2)]) == True:\n",
    "            if p1 != p2:\n",
    "                sim_res = self.similarity_calculate(p1, p2)\n",
    "                self.similarities.at[p1, str(p2)] = sim_res\n",
    "                self.similarities.at[p2, str(p1)] = sim_res\n",
    "            else:\n",
    "                return np.nan\n",
    "        \n",
    "        return self.similarities.at[p1, str(p2)]\n",
    "    \n",
    "    \n",
    "    def similarity_calculate(self, p1, p2):\n",
    "        \n",
    "        data = self.X.data\n",
    "        \n",
    "        # Get all users that have rated p1 or p2:\n",
    "        relevant_data = data[(data['movieID'] == p1) | (data['movieID'] == p2) ][['userID', 'movieID']]\n",
    "        # Get only those users that have rated BOTH p1 and p2 movies:\n",
    "        relevant_data = relevant_data[relevant_data.groupby(['userID'])['userID'].transform('count') == 2]\n",
    "        # Get unuqie column (table) of users:\n",
    "        relevant_data = relevant_data[['userID']].drop_duplicates()\n",
    "        \n",
    "        table = pd.DataFrame(columns = ['userID', 'avgRating', 'movie1_diff', 'movie2_diff'])\n",
    "        table['userID'] = relevant_data['userID']\n",
    "        table = table.reset_index(drop = True)\n",
    "        \n",
    "        if len(table.index) < self.min_values:\n",
    "#             print('min_values applied:')\n",
    "            return float(0)\n",
    "        \n",
    "        table['avgRating'] = np.vectorize(self.find_avgRating)(table['userID'])\n",
    "        table['movie1_diff'] = np.vectorize(self.getRatingDifference)(table['userID'], table['avgRating'], p1)\n",
    "        table['movie2_diff'] = np.vectorize(self.getRatingDifference)(table['userID'], table['avgRating'], p2)\n",
    "        table['diffs_product'] = np.vectorize(self.cols_product)(table['movie1_diff'], table['movie2_diff'])\n",
    "        \n",
    "        numenator = table['diffs_product'].sum()\n",
    "        \n",
    "        table['part1'] = np.vectorize(self.cols_product)(table['movie1_diff'], table['movie1_diff'])\n",
    "        part1 = table['part1'].sum()\n",
    "        part1 = part1 ** 0.5 # square root\n",
    "        \n",
    "        table['part2'] = np.vectorize(self.cols_product)(table['movie2_diff'], table['movie2_diff'])\n",
    "        part2 = table['part2'].sum()\n",
    "        part2 = part2 ** 0.5 # square root\n",
    "        \n",
    "        denominator = part1 * part2\n",
    "        result = numenator / denominator\n",
    "        \n",
    "        if result < self.threshold:\n",
    "#             print('threshold applied:')\n",
    "            result = 0\n",
    "    \n",
    "        return float(result)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def similarItems(self, item, n):\n",
    "        \n",
    "        movie_column = pd.DataFrame(columns = [str(item)])\n",
    "        movie_column[str(item)] = self.similarities[str(item)]\n",
    "#         display(movie_column)\n",
    "        \n",
    "        simil_dict = movie_column.to_dict()\n",
    "#         print(simil_dict)\n",
    "                    \n",
    "        simil_list = list(simil_dict[str(item)].items())\n",
    "        \n",
    "        simil_list_sorted = sorted(simil_list, key = lambda x: x[1], reverse = True) # sort from biggest similarity to the lowest\n",
    "        simil_list_sorted = [(x, y) for x, y in simil_list_sorted if pd.isna(y) == False] # remove NaN values\n",
    "        simil_list_sorted_the_best = simil_list_sorted[0:n] # note that we include a pointer to the next value, which is n (the last value in our list will be with index n-1, although here we see n)\n",
    "        \n",
    "        return simil_list_sorted_the_best\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "md = MovieData('data/movies.dat')\n",
    "uim = UserItemData('data/user_ratedmovies.dat', min_ratings=1000)\n",
    "rp = ItemBasedPredictor()\n",
    "rec = Recommender(rp)\n",
    "rec.fit(uim)\n",
    "\n",
    "print('Calculating similarities: ')\n",
    "print(\"Similarity between the movies 'Men in black'(1580) and 'Ghostbusters'(2716): \", rp.similarity(1580, 2716))\n",
    "print(\"Similarity between the movies 'Men in black'(1580) and 'Schindler's List'(527): \", rp.similarity(1580, 527))\n",
    "print(\"Similarity between the movies 'Men in black'(1580) and 'Independence day'(780): \", rp.similarity(1580, 780))\n",
    "\n",
    "print('Now, we will NOT calculate it, but simply GET in from the table:')\n",
    "print(\"Similarity between the movies 'Men in black'(1580) and 'Ghostbusters'(2716): \", rp.similarity(1580, 2716))\n",
    "print(\"Similarity between the movies 'Men in black'(1580) and 'Schindler's List'(527): \", rp.similarity(1580, 527))\n",
    "print(\"Similarity between the movies 'Men in black'(1580) and 'Independence day'(780): \", rp.similarity(1580, 780))\n",
    "\n",
    "print('Checking vice-versa coordinates:')\n",
    "print(\"Similarity between the movies 'Men in black'(1580) and 'Ghostbusters'(2716): \", rp.similarity(2716, 1580))\n",
    "print(\"Similarity between the movies 'Men in black'(1580) and 'Schindler's List'(527): \", rp.similarity(527, 1580))\n",
    "print(\"Similarity between the movies 'Men in black'(1580) and 'Independence day'(780): \", rp.similarity(780, 1580))\n",
    "\n",
    "\n",
    "# New run:\n",
    "\n",
    "md = MovieData('data/movies.dat')\n",
    "uim = UserItemData('data/user_ratedmovies.dat', min_ratings=1000)\n",
    "rp = ItemBasedPredictor()\n",
    "rec = Recommender(rp)\n",
    "rec.fit(uim)\n",
    "\n",
    "# In case you do not want to wait for the calculations, you can UNcomment these two lines\n",
    "# and you would simply read the calculations that were done BEFORE:\n",
    "# rp.similarities = pd.read_table('output/calculated_similarities.dat', index_col = 0)\n",
    "# display(rp.similarities)\n",
    "\n",
    "print(\"Predictions for 78: \")\n",
    "rec_items = rec.recommend(78, n=15, rec_seen=False)\n",
    "for idmovie, val in rec_items:\n",
    "    print(\"Movie: {}, score: {}\".format(md.get_title(idmovie), val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(rp.similarities)\n",
    "# # That's how we should save the similarities so that they can help us in the future:\n",
    "# rp.similarities.to_csv('output/calculated_similarities_copy.dat', sep = '\\t', index=True)\n",
    "# # That's how we should read them:\n",
    "# saved_similarities = pd.read_table('output/calculated_similarities_copy.dat', index_col = 0)\n",
    "# display(saved_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Most similar movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie 1: The Lord of the Rings: The Two Towers, Movie2: The Lord of the Rings: The Return of the King, Similarity: 0.8439842148481417\n",
      "Movie 1: The Lord of the Rings: The Return of the King, Movie2: The Lord of the Rings: The Two Towers, Similarity: 0.8439842148481417\n",
      "Movie 1: The Lord of the Rings: The Fellowship of the Ring, Movie2: The Lord of the Rings: The Two Towers, Similarity: 0.8231885401761888\n",
      "Movie 1: The Lord of the Rings: The Two Towers, Movie2: The Lord of the Rings: The Fellowship of the Ring, Similarity: 0.8231885401761888\n",
      "Movie 1: The Lord of the Rings: The Fellowship of the Ring, Movie2: The Lord of the Rings: The Return of the King, Similarity: 0.8079374897442496\n",
      "Movie 1: The Lord of the Rings: The Return of the King, Movie2: The Lord of the Rings: The Fellowship of the Ring, Similarity: 0.8079374897442496\n",
      "Movie 1: Kill Bill: Vol. 2, Movie2: Kill Bill: Vol. 2, Similarity: 0.7372340224381029\n",
      "Movie 1: Kill Bill: Vol. 2, Movie2: Kill Bill: Vol. 2, Similarity: 0.7372340224381029\n",
      "Movie 1: Star Wars, Movie2: Star Wars: Episode V - The Empire Strikes Back, Similarity: 0.7021321132220318\n",
      "Movie 1: Star Wars: Episode V - The Empire Strikes Back, Movie2: Star Wars, Similarity: 0.7021321132220318\n",
      "Movie 1: Ace Ventura: Pet Detective, Movie2: The Mask, Similarity: 0.6616471778494046\n",
      "Movie 1: The Mask, Movie2: Ace Ventura: Pet Detective, Similarity: 0.6616471778494046\n",
      "Movie 1: Star Wars: Episode V - The Empire Strikes Back, Movie2: Star Wars: Episode VI - Return of the Jedi, Similarity: 0.5992253753778948\n",
      "Movie 1: Star Wars: Episode VI - Return of the Jedi, Movie2: Star Wars: Episode V - The Empire Strikes Back, Similarity: 0.5992253753778948\n",
      "Movie 1: Independence Day, Movie2: Star Wars: Episode I - The Phantom Menace, Similarity: 0.5610426219249997\n",
      "Movie 1: Star Wars: Episode I - The Phantom Menace, Movie2: Independence Day, Similarity: 0.5610426219249997\n",
      "Movie 1: Ace Ventura: Pet Detective, Movie2: Austin Powers: The Spy Who Shagged Me, Similarity: 0.5546511205201551\n",
      "Movie 1: Austin Powers: The Spy Who Shagged Me, Movie2: Ace Ventura: Pet Detective, Similarity: 0.5546511205201551\n",
      "Movie 1: Star Wars, Movie2: Star Wars: Episode VI - Return of the Jedi, Similarity: 0.5537849318137372\n",
      "Movie 1: Star Wars: Episode VI - Return of the Jedi, Movie2: Star Wars, Similarity: 0.5537849318137372\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate all remaining similarities:\n",
    "\n",
    "# In case you do not want to wait for the calculations, you can UNcomment these two lines\n",
    "# and you would simply read the calculations that were done BEFORE:\n",
    "# rp.similarities = pd.read_table('output/all_calculated_similarities.dat', index_col = 0)\n",
    "# display(rp.similarities)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for column in rp.similarities: # iterate through every column to calculate remaining similarities:\n",
    "    rp.similarities[column] = np.vectorize(rp.similarity)(rp.similarities.index, int(column))\n",
    "\n",
    "end = time.time()\n",
    "time_spent = end - start\n",
    "# print('Time needed for remaining calculations of similarities: ' +str(time_spent))\n",
    "\n",
    "rp.similarities.to_csv('output/all_calculated_similarities.dat', sep = '\\t', index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now, let's play with similarities when we have all of them:\n",
    "\n",
    "simil_dict = rp.similarities.to_dict()\n",
    "simil_list = []\n",
    "\n",
    "for k1, v1 in simil_dict.items():\n",
    "    for k2, v2 in v1.items():\n",
    "        if pd.isna(v2) == False:\n",
    "            # int(k1) because it is column, which means it's a string, but we want integer for simplicity:\n",
    "            local_tuple = (int(k1), k2, v2)\n",
    "            simil_list.append(local_tuple)\n",
    "\n",
    "simil_list_sorted = sorted(simil_list, key=lambda x: x[2], reverse = True) # sort from biggest similarity to the lowest\n",
    "\n",
    "for i in range(20):\n",
    "    print('Movie 1: ' +md.get_title(simil_list_sorted[i][0])+ ', Movie2: ' +md.get_title(simil_list_sorted[i][1])+ ', Similarity: ' +str(simil_list_sorted[i][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Recommendation based on the currently viewed content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies similar to \"The Lord of the Rings: The Fellowship of the Ring\": \n",
      "Movie: The Lord of the Rings: The Two Towers, score: 0.8231885401761888\n",
      "Movie: The Lord of the Rings: The Return of the King, score: 0.8079374897442496\n",
      "Movie: Star Wars: Episode V - The Empire Strikes Back, score: 0.2396194307349645\n",
      "Movie: Star Wars, score: 0.2196558652707407\n",
      "Movie: The Matrix, score: 0.2151555270688023\n",
      "Movie: Raiders of the Lost Ark, score: 0.19944276706345015\n",
      "Movie: The Usual Suspects, score: 0.18321188451910753\n",
      "Movie: Blade Runner, score: 0.16399681315410275\n",
      "Movie: Schindler's List, score: 0.16105905138148702\n",
      "Movie: Monty Python and the Holy Grail, score: 0.15780453798519137\n"
     ]
    }
   ],
   "source": [
    "rec_items = rp.similarItems(4993, 10)\n",
    "print('Movies similar to \"The Lord of the Rings: The Fellowship of the Ring\": ')\n",
    "for idmovie, val in rec_items:\n",
    "    print(\"Movie: {}, score: {}\".format(md.get_title(idmovie), val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Recommendation for yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for 78: \n",
      "Time needed for movieID 32: 32.953595876693726 seconds.\n",
      "Time needed for movieID 110: 34.130043745040894 seconds.\n",
      "Time needed for movieID 296: 42.17946410179138 seconds.\n",
      "Time needed for movieID 589: 38.13544940948486 seconds.\n",
      "Time needed for movieID 1036: 30.97529911994934 seconds.\n",
      "Time needed for movieID 1527: 33.079095125198364 seconds.\n",
      "Time needed for movieID 2571: 41.14597487449646 seconds.\n",
      "Time needed for movieID 2762: 42.21225547790527 seconds.\n",
      "Time needed for movieID 2959: 31.552284002304077 seconds.\n",
      "Time needed for movieID 3793: 31.594585418701172 seconds.\n",
      "Time needed for movieID 4993: 29.439985036849976 seconds.\n",
      "Time needed for movieID 5952: 24.118951082229614 seconds.\n",
      "Time needed for movieID 7153: 19.319316625595093 seconds.\n",
      "Time needed for movieID 32587: 29.230201959609985 seconds.\n",
      "Time needed for movieID 150: 28.77849817276001 seconds.\n",
      "Time needed for movieID 260: 16.42332935333252 seconds.\n",
      "Time needed for movieID 318: 11.893150568008423 seconds.\n",
      "Time needed for movieID 356: 40.75067400932312 seconds.\n",
      "Time needed for movieID 480: 38.151461601257324 seconds.\n",
      "Time needed for movieID 527: 30.652432918548584 seconds.\n",
      "Time needed for movieID 541: 28.18293595314026 seconds.\n",
      "Time needed for movieID 608: 29.200926303863525 seconds.\n",
      "Time needed for movieID 780: 36.01005029678345 seconds.\n",
      "Time needed for movieID 858: 32.894126892089844 seconds.\n",
      "Time needed for movieID 1089: 28.531081438064575 seconds.\n",
      "Time needed for movieID 1097: 29.775463104248047 seconds.\n",
      "Time needed for movieID 1136: 31.223960876464844 seconds.\n",
      "Time needed for movieID 1196: 9.28568434715271 seconds.\n",
      "Time needed for movieID 1198: 35.33484888076782 seconds.\n",
      "Time needed for movieID 1210: 5.633073329925537 seconds.\n",
      "Time needed for movieID 1214: 29.62226176261902 seconds.\n",
      "Time needed for movieID 1240: 32.07407903671265 seconds.\n",
      "Time needed for movieID 1265: 37.100335359573364 seconds.\n",
      "Time needed for movieID 1270: 46.77604627609253 seconds.\n",
      "Time needed for movieID 1682: 38.43000245094299 seconds.\n",
      "Time needed for movieID 1923: 35.452483892440796 seconds.\n",
      "Time needed for movieID 2028: 42.27979278564453 seconds.\n",
      "Time needed for movieID 2716: 39.056156635284424 seconds.\n",
      "Time needed for movieID 2858: 46.78063130378723 seconds.\n",
      "Time needed for movieID 2997: 35.79972434043884 seconds.\n",
      "Time needed for movieID 3996: 37.53405213356018 seconds.\n",
      "Time needed for movieID 4226: 41.93181228637695 seconds.\n",
      "Time needed for movieID 4973: 35.30364274978638 seconds.\n",
      "Time needed for movieID 5445: 41.815855741500854 seconds.\n",
      "Time needed for movieID 6365: 3.0690269470214844 seconds.\n",
      "Time needed for movieID 6711: 35.95286011695862 seconds.\n",
      "Time needed for movieID 6874: 42.300952434539795 seconds.\n",
      "Time needed for movieID 7361: 38.168636083602905 seconds.\n",
      "Time needed for movieID 7438: 39.49820137023926 seconds.\n",
      "Time needed for movieID 1: 41.783376693725586 seconds.\n",
      "Time needed for movieID 47: 40.229432582855225 seconds.\n",
      "Time needed for movieID 50: 39.355833530426025 seconds.\n",
      "Time needed for movieID 344: 34.47688937187195 seconds.\n",
      "Time needed for movieID 364: 35.178192377090454 seconds.\n",
      "Time needed for movieID 367: 36.8168511390686 seconds.\n",
      "Time needed for movieID 377: 36.99009561538696 seconds.\n",
      "Time needed for movieID 500: 29.45262908935547 seconds.\n",
      "Time needed for movieID 597: 30.7133150100708 seconds.\n",
      "Time needed for movieID 648: 36.504703998565674 seconds.\n",
      "Time needed for movieID 1580: 45.28541922569275 seconds.\n",
      "Time needed for movieID 1721: 42.20348501205444 seconds.\n",
      "Time needed for movieID 2683: 32.71085476875305 seconds.\n",
      "Time needed for movieID 4306: 42.509878158569336 seconds.\n",
      "Time needed for movieID 4886: 30.831462383270264 seconds.\n",
      "Time needed for movieID 4963: 33.02098083496094 seconds.\n",
      "Time needed for movieID 6377: 33.339574575424194 seconds.\n",
      "Time needed for movieID 457: 29.88503098487854 seconds.\n",
      "Time needed for movieID 592: 30.759546995162964 seconds.\n",
      "Time needed for movieID 593: 38.05335593223572 seconds.\n",
      "Time needed for movieID 1961: 28.628812789916992 seconds.\n",
      "Time needed for movieID 2628: 0.004328012466430664 seconds.\n",
      "Time needed for movieID 3578: 36.64574408531189 seconds.\n",
      "Time needed for movieID 5418: 30.67031741142273 seconds.\n",
      "Time needed for movieID 33794: 33.07946252822876 seconds.\n",
      "Time needed for movieID 1291: 32.20017671585083 seconds.\n",
      "Time needed for movieID 4995: 30.67581081390381 seconds.\n",
      "Time needed for movieID 5349: 34.111308336257935 seconds.\n",
      "Time needed for movieID 5989: 29.07939076423645 seconds.\n",
      "Time needed for movieID 6539: 36.46790385246277 seconds.\n",
      "Time needed for movieID 8961: 31.68055772781372 seconds.\n",
      "Time needed for movieID 1704: 29.089165210723877 seconds.\n",
      "Time spent for the whole RECOMMENDATION: 2664.1878247261047 seconds.\n",
      "Movie: Rain Man, score: 4.998089136637091\n",
      "Movie: Forrest Gump, score: 4.926465291538581\n",
      "Movie: A Beautiful Mind, score: 4.8838554407883015\n",
      "Movie: Good Will Hunting, score: 4.8819023479553625\n",
      "Movie: Finding Nemo, score: 4.850471565039435\n",
      "Movie: The Sixth Sense, score: 4.813605882720601\n",
      "Movie: Braveheart, score: 4.797399135738161\n",
      "Movie: Gladiator, score: 4.789642362646861\n",
      "Movie: American Beauty, score: 4.766524534552269\n",
      "Movie: The Truman Show, score: 4.755429214839215\n",
      "Movie: Saving Private Ryan, score: 4.748288688447756\n",
      "Movie: Shichinin no samurai, score: 4.7339443515228865\n",
      "Movie: The Silence of the Lambs, score: 4.732550140687089\n",
      "Movie: Shrek, score: 4.720413959914994\n",
      "Movie: Le fabuleux destin d'Amélie Poulain, score: 4.713604168084137\n"
     ]
    }
   ],
   "source": [
    "md_1 = MovieData('data/movies.dat')\n",
    "uim_1 = UserItemData('data/user_ratedmovies_additional.dat', min_ratings=1000)\n",
    "rp_1 = ItemBasedPredictor()\n",
    "rec_1 = Recommender(rp_1)\n",
    "rec_1.fit(uim_1)\n",
    "\n",
    "# In case you do not want to wait for the calculations, you can UNcomment these two lines\n",
    "# and you would simply read the calculations that were done BEFORE:\n",
    "# rp_1.similarities = pd.read_table('output/all_calculated_similarities.dat', index_col = 0)\n",
    "# display(rp_1.similarities)\n",
    "\n",
    "print(\"Predictions for 78: \")\n",
    "rec_items_1 = rec_1.recommend(888888, n=15, rec_seen=False)\n",
    "for idmovie, val in rec_items_1:\n",
    "    print(\"Movie: {}, score: {}\".format(md.get_title(idmovie), val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Prediction with Slope One method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for 78: \n",
      "Movie: The Usual Suspects, score: 4.325079182263173\n",
      "Movie: The Lord of the Rings: The Fellowship of the Ring, score: 4.155293229840448\n",
      "Movie: The Lord of the Rings: The Return of the King, score: 4.153135076202185\n",
      "Movie: The Silence of the Lambs, score: 4.127978169643881\n",
      "Movie: Shichinin no samurai, score: 4.119790444913598\n",
      "Movie: The Lord of the Rings: The Two Towers, score: 4.083325894849594\n",
      "Movie: Indiana Jones and the Last Crusade, score: 3.9670398355464194\n",
      "Movie: The Incredibles, score: 3.9664496674557546\n",
      "Movie: Good Will Hunting, score: 3.963362387354114\n",
      "Movie: Sin City, score: 3.942619137615212\n",
      "Movie: Batman Begins, score: 3.9375326640077017\n",
      "Movie: A Beautiful Mind, score: 3.9140940935239508\n",
      "Movie: Rain Man, score: 3.9107819079644943\n",
      "Movie: Monsters, Inc., score: 3.8819375978658006\n",
      "Movie: Finding Nemo, score: 3.8807711131654794\n"
     ]
    }
   ],
   "source": [
    "class SlopeOnePredictor:\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \n",
    "        self.X = X\n",
    "        \n",
    "        self.deviations = pd.DataFrame(index = self.X.data.movieID.unique())\n",
    "        for i in self.deviations.index:\n",
    "            self.deviations[str(i)] = np.nan\n",
    "        \n",
    "        self.deviations_count = pd.DataFrame(index = self.X.data.movieID.unique())\n",
    "        for i in self.deviations_count.index:\n",
    "            self.deviations_count[str(i)] = np.nan\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_deviation_denominator(self, p1, p2):\n",
    "        return self.deviations_count.at[p1, str(p2)]\n",
    "    \n",
    "    def calc_movies_diff(self, userID, p1, p2):\n",
    "        \n",
    "        movie_1 = self.X.data.loc[((self.X.data.userID == userID) & (self.X.data.movieID == p1)), 'rating'].iloc[0]\n",
    "        movie_2 = self.X.data.loc[((self.X.data.userID == userID) & (self.X.data.movieID == p2)), 'rating'].iloc[0]\n",
    "        return movie_1 - movie_2\n",
    "    \n",
    "    \n",
    "    def numerator_part(self, col1, col2, userID, movieID):\n",
    "        rating = self.X.data.loc[((self.X.data.userID == userID) & (self.X.data.movieID == movieID)), 'rating'].iloc[0]\n",
    "        return (col1 + rating) * col2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, userID): # а вот теперь мы этот userID наконец-то юзаем, гы\n",
    "        \n",
    "        start_time_prediction = time.time()\n",
    "        recommend_dict = {}\n",
    "        \n",
    "        unique_movies = self.X.data.movieID.unique()\n",
    "#         display(unique_movies)\n",
    "        \n",
    "        for i in unique_movies: # iterate though the list of unique movieID-s that appear in this particular data\n",
    "            \n",
    "            start_time_movie = time.time()\n",
    "            \n",
    "            personal = self.X.data[self.X.data.userID == userID][['movieID']]\n",
    "            personal['deviation'] = np.nan\n",
    "\n",
    "            remove_index = personal[personal.movieID == i].index\n",
    "#             print('Next index to remove: ' +str(remove_index))\n",
    "            personal = personal.drop(remove_index)\n",
    "\n",
    "            personal['deviation'] = np.vectorize(self.deviation)(i, personal['movieID'])\n",
    "            personal['deviation_denominator'] = np.vectorize(self.get_deviation_denominator)(i, personal['movieID'])\n",
    "            personal['deviation_numerator_prod'] = np.vectorize(self.numerator_part)(personal['deviation'], personal['deviation_denominator'], userID, personal['movieID'])\n",
    "            \n",
    "#             display(personal)\n",
    "            numerator = personal['deviation_numerator_prod'].sum()\n",
    "            denominator = personal['deviation_denominator'].sum()\n",
    "\n",
    "            result = numerator / denominator\n",
    "#             print('The recommentation for movieID ' +str(i)+ ' is: ' +str(result))\n",
    "            \n",
    "            recommend_dict[i] = result\n",
    "            \n",
    "            end_time_movie = time.time()\n",
    "            time_movie = end_time_movie - start_time_movie\n",
    "#             print('Time needed for movieID ' +str(i)+ ': ' +str(time_movie)+ ' seconds.')\n",
    "        \n",
    "        end_time_prediction = time.time()\n",
    "        time_prediction = end_time_prediction - start_time_prediction\n",
    "#         print('Time spent for the whole RECOMMENDATION: ' +str(time_prediction)+ ' seconds.')\n",
    "        \n",
    "        # Save similarities to use it in the future...\n",
    "        # Index must be true here so that we can further read the firct column as index column:\n",
    "        self.deviations.to_csv('output/calculated_deviations.dat', sep = '\\t', index=True)\n",
    "        self.deviations_count.to_csv('output/calculated_deviations_count.dat', sep = '\\t', index=True)\n",
    "        \n",
    "        return recommend_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def deviation(self, p1, p2):\n",
    "        \n",
    "        # Check if this is NaN value:\n",
    "        if pd.isna(self.deviations.at[p1, str(p2)]) == True:\n",
    "            if p1 != p2:\n",
    "                dev_res = self.deviation_calculate(p1, p2)\n",
    "                self.deviations.at[p1, str(p2)] = dev_res\n",
    "                self.deviations.at[p2, str(p1)] = -(dev_res) # negative !\n",
    "            else:\n",
    "                return np.nan\n",
    "        \n",
    "        return self.deviations.at[p1, str(p2)]\n",
    "    \n",
    "    \n",
    "    def deviation_calculate(self, p1, p2):\n",
    "        \n",
    "        data = self.X.data\n",
    "        \n",
    "        # Get all users that have rated p1 or p2:\n",
    "        relevant_data = data[(data['movieID'] == p1) | (data['movieID'] == p2) ][['userID', 'movieID']]\n",
    "        # Get only those users that have rated BOTH p1 and p2 movies:\n",
    "        relevant_data = relevant_data[relevant_data.groupby(['userID'])['userID'].transform('count') == 2]\n",
    "        # Get unuqie column (table) of users:\n",
    "        relevant_data = relevant_data[['userID']].drop_duplicates()\n",
    "        \n",
    "        table = pd.DataFrame(columns = ['userID', 'movies_diff'])\n",
    "        table['userID'] = relevant_data['userID']\n",
    "        table = table.reset_index(drop = True)\n",
    "        \n",
    "        table['movies_diff'] = np.vectorize(self.calc_movies_diff)(table['userID'], p1, p2)\n",
    "        \n",
    "        numerator = table['movies_diff'].sum()\n",
    "        denominator = table['movies_diff'].count() # so this is basically just a counter of how many sums we get\n",
    "        \n",
    "        self.deviations_count.at[p1, str(p2)] = denominator\n",
    "        self.deviations_count.at[p2, str(p1)] = denominator # this one is positive in comparison with the deviation itself !\n",
    "        \n",
    "        result = float(numerator / denominator)\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "md_2 = MovieData('data/movies.dat')\n",
    "uim_2 = UserItemData('data/user_ratedmovies.dat', min_ratings=1000)\n",
    "rp_2 = SlopeOnePredictor()\n",
    "rec_2 = Recommender(rp_2)\n",
    "rec_2.fit(uim_2)\n",
    "\n",
    "# In case you do not want to wait for the calculations, you can UNcomment these four lines\n",
    "# and you would simply read the calculations that were done BEFORE:\n",
    "# rp_2.deviations = pd.read_table('output/calculated_deviations.dat', index_col = 0)\n",
    "# display(rp_2.deviations)\n",
    "# rp_2.deviations_count = pd.read_table('output/calculated_deviations_count.dat', index_col = 0)\n",
    "# display(rp_2.deviations_count)\n",
    "\n",
    "print(\"Predictions for 78: \")\n",
    "rec_items_2 = rec_2.recommend(78, n=15, rec_seen=False)\n",
    "for idmovie, val in rec_items_2:\n",
    "    print(\"Movie: {}, score: {}\".format(md.get_title(idmovie), val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
